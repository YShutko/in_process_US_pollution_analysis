{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48a6c7b",
   "metadata": {},
   "source": [
    "# <center> US pollution analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87597185",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb91f00",
   "metadata": {},
   "source": [
    "The US Pollution Data project leverages a comprehensive dataset covering air pollution measurements across the United States ‚Äî consisting of over 1.4‚ÄØmillion observations and around 28 variables that record concentrations of major pollutants such as nitrogen dioxide (NO‚ÇÇ), sulfur dioxide (SO‚ÇÇ), carbon monoxide (CO), and ozone (O‚ÇÉ). \n",
    "\n",
    "\n",
    "This dataset spans several years and states, allowing detailed spatio-temporal analysis of pollutant levels, seasonal trends, and geographic variation. The primary objective of this project is to transform this raw data into clean, analysis-ready formats (e.g., Parquet), conduct exploratory data analysis (EDA) to uncover patterns and insights, and ‚Äî where possible ‚Äî apply machine learning techniques to forecast pollutant concentrations, classify air‚Äëquality levels, or identify key factors driving pollution.\n",
    "\n",
    "Given the public health importance of air quality, this project has the potential not only to improve our understanding of pollution trends in the U.S., but also to inform policy, raise awareness, or support predictive systems that warn populations about deteriorating air conditions.\n",
    "\n",
    "The goal of this project is to:\n",
    "\n",
    "* Clean and transform the raw data (e.g., into Parquet)\n",
    "* Perform EDA to uncover trends, seasonal patterns, and geographic variation\n",
    "* Apply ML techniques to forecast pollutant levels, classify air quality, and analyze feature importance\n",
    "\n",
    "By improving our understanding of air quality trends, this project supports public health insights and data-driven policy decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27caff",
   "metadata": {},
   "source": [
    "Dataset content:\n",
    "* State Code: Numeric code representing the U.S. state\n",
    "* County Code:\tNumeric code for the county within the state\n",
    "* Site Num:\tIdentifier for the air monitoring site\n",
    "* Address:\tStreet address of the monitoring station\n",
    "* State:\tFull name of the U.S. state\n",
    "* County:\tName of the county\n",
    "* City:\tCity where the measurement site is located\n",
    "* Date Local:\tDate of the observation (YYYY-MM-DD)\n",
    "* NO2 Units:\tUnits used for nitrogen dioxide measurements\n",
    "* NO2 Mean:\tDaily average NO‚ÇÇ concentration\n",
    "* NO2 1st Max Value:\tHighest NO‚ÇÇ value recorded that day\n",
    "* NO2 1st Max Hour:\tHour when the highest NO‚ÇÇ was recorded\n",
    "* NO2 AQI:\tAir Quality Index for NO‚ÇÇ on that day\n",
    "* O3 Units:\tUnits used for ozone measurements\n",
    "* O3 Mean:\tDaily average ozone concentration\n",
    "* O3 1st Max Value:\tHighest O‚ÇÉ value recorded that day\n",
    "* O3 1st Max Hour:\tHour when the highest O‚ÇÉ was recorded\n",
    "* O3 AQI:\tAir Quality Index for O‚ÇÉ on that day\n",
    "* SO2 Units:\tUnits used for sulfur dioxide measurements\n",
    "* SO2 Mean:\tDaily average SO‚ÇÇ concentration\n",
    "* SO2 1st Max Value:\tHighest SO‚ÇÇ value recorded that day\n",
    "* SO2 1st Max Hour:\tHour when the highest SO‚ÇÇ was recorded\n",
    "* SO2 AQI:\tAir Quality Index for SO‚ÇÇ on that day\n",
    "* CO Units:\tUnits used for carbon monoxide measurements\n",
    "* CO Mean:\tDaily average CO concentration\n",
    "* CO 1st Max Value:\tHighest CO value recorded that day\n",
    "* CO 1st Max Hour:\tHour when the highest CO was recorded\n",
    "* CO AQI:\tAir Quality Index for CO on that day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41336fab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820cde0",
   "metadata": {},
   "source": [
    "### 1. EDA and Initial data visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990d4f59",
   "metadata": {},
   "source": [
    "First, all necessary libraries are imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba364f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                          #import os for operating system interactions\n",
    "import pandas as pd                 #import Pandas for data manipulation\n",
    "import numpy as np                  #import Numpy for numerical operations\n",
    "import matplotlib.pyplot as plt     #import Matplotlib for data visualization\n",
    "import seaborn as sns               #import Seaborn for statistical data visualization\n",
    "from plotly.subplots import make_subplots  #import Plotly subplots for creating complex figures\n",
    "import plotly.express as px         #import Plotly Express for interactive visualizations\n",
    "import plotly.graph_objects as go   #import Plotly Graph Objects for detailed figure customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa936027",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")                  # Set Seaborn style for plots\n",
    "plt.rcParams[\"figure.figsize\"] = (10,6)     # Set default figure size for Matplotlib plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acf5f30",
   "metadata": {},
   "source": [
    "#### 1.1. ETL and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf2988a",
   "metadata": {},
   "source": [
    "\n",
    "In this section EDA, including data load and cleaning, is performed. As a first step, data set is loaded into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add73d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/raw/pollution_dataset.parquet\", engine=\"pyarrow\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89622488",
   "metadata": {},
   "outputs": [],
   "source": [
    "df. drop(columns=['Unnamed: 0'], inplace=True)  # Drop unnecessary column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a57add5",
   "metadata": {},
   "source": [
    "In the following subsection initial data set inspection is performed. Here the shape and Info of DataFrame are shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a277b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)                     # Print the shape of the DataFrame           \n",
    "print(df.info())                    # Print concise summary of the DataFrame            \n",
    "print(df.dtypes)                    # Print data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5521757",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes.value_counts()         # Count occurrences of each data type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f29624",
   "metadata": {},
   "source": [
    "As it shown above dataset consists of 1746661 entries and 28 columns. Also dataset contains 10 float columns, 9 integer and 9 categorical columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e193ff",
   "metadata": {},
   "source": [
    "In the next steps DataFrame is checked for any incosistencies(dublicates, missing value and etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()           # Check for missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26e8ef",
   "metadata": {},
   "source": [
    "The dataset misses 872907 and 873323 values in SO2 AQI and CO AQI columns respectively. This is a big amount of missing data to just remove lines. Instead, these missing values can be calculated, as far as other columns have no missing points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6c5e8",
   "metadata": {},
   "source": [
    "Air Quality Index (AQI) is calculated by converting measured pollutant concentrations (e.g., SO‚ÇÇ, CO, NO‚ÇÇ, O‚ÇÉ, PM‚ÇÇ.‚ÇÖ, PM‚ÇÅ‚ÇÄ) into a standardized scale (usually 0‚Äì500) using breakpoints.\n",
    "Core AQI Formula\n",
    "\n",
    "Each pollutant gets its own AQI number. The final AQI for the city/location is the highest of all pollutants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdadd55",
   "metadata": {},
   "source": [
    "Each government sets concentration ranges for each pollutant.\n",
    "Example (US EPA standard):\n",
    "SO‚ÇÇ 1-hour breakpoints (ppb)\n",
    "AQI Range\t                SO‚ÇÇ (ppb)\n",
    "0‚Äì50 (Good)\t                0‚Äì35\n",
    "51‚Äì100 (Moderate)\t        36‚Äì75\n",
    "101‚Äì150 (Unhealthy SG)\t    76‚Äì185\n",
    "151‚Äì200 (Unhealthy)\t        186‚Äì304\n",
    "201‚Äì300 (Very Unhealthy)    305‚Äì604"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141446a7",
   "metadata": {},
   "source": [
    "CO 8-hour breakpoints (ppm)\n",
    "AQI Range\tCO (ppm)\n",
    "0‚Äì50\t    0.0‚Äì4.4\n",
    "51‚Äì100\t    4.5‚Äì9.4\n",
    "101‚Äì150\t    9.5‚Äì12.4\n",
    "151‚Äì200\t    12.5‚Äì15.4\n",
    "201‚Äì300\t    15.5‚Äì30.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c736d",
   "metadata": {},
   "source": [
    "Before calculating AQIs SO2 and CO, let's confirm that dedicated cilumns do not contain negeative values (which is physically impossible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_so2 = df[df[\"SO2 1st Max Value\"] < 0][[\"SO2 Units\", \"SO2 Mean\", \"SO2 1st Max Value\"]]\n",
    "neg_co = df[df[\"CO Mean\"] < 0][[\"CO Units\", \"CO Mean\", \"CO 1st Max Value\"]]\n",
    "\n",
    "print(\"Negative SO‚ÇÇ values:\")\n",
    "print(neg_so2.head())\n",
    "\n",
    "print(\"\\nNegative CO values:\")\n",
    "print(neg_co.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8906cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neg_co.value_counts().sum())\n",
    "print(neg_so2.value_counts().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae367b",
   "metadata": {},
   "source": [
    "There are 1064 and 8286 negative values in CO Mean and SO2 1st Max Value columns, respectively. In this case, removing these values from the dataset is the simplest, cleanest and safest approach.\n",
    "\n",
    "Negative pollution values are invalid. SO‚ÇÇ and CO cannot be negative in reality. These values come from:\n",
    "* sensor malfunction\n",
    "* data ingestion error\n",
    "* interpolation issues\n",
    "\n",
    "Removing them does not lose valid information.\n",
    "\n",
    "Only a tiny fraction of data is affected. There are 1,746,661 total rows. Problematic rows:\n",
    "* SO‚ÇÇ negatives: 8,286\n",
    "* CO negatives: 1,064\n",
    "* Combined: < 0.5% of data\n",
    "\n",
    "Removing them has zero statistical impact on AQI analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a93e487",
   "metadata": {},
   "source": [
    "Deleting invalid rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd5d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df[\"SO2 1st Max Value\"] >= 0) & (df[\"CO Mean\"] >= 0)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699411ed",
   "metadata": {},
   "source": [
    "Below SO2 AQI column calculated based on given values in dedicated SO2 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_so2_aqi(C):\n",
    "    \"\"\"\n",
    "    Calculate SO2 AQI using expanded breakpoint intervals (Option C).\n",
    "    This avoids NA values from strict EPA bins.\n",
    "    \n",
    "    C : float \n",
    "        1-hour SO2 concentration in ppb\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(C):\n",
    "        return np.nan\n",
    "\n",
    "    # ===== Expanded breakpoints ensuring continuous coverage =====\n",
    "    if 0 <= C <= 35.999:\n",
    "        Clow, Chigh = 0, 35\n",
    "        Ilow, Ihigh = 0, 50\n",
    "\n",
    "    elif 36 <= C <= 75.999:\n",
    "        Clow, Chigh = 36, 75\n",
    "        Ilow, Ihigh = 51, 100\n",
    "\n",
    "    elif 76 <= C <= 185.999:\n",
    "        Clow, Chigh = 76, 185\n",
    "        Ilow, Ihigh = 101, 150\n",
    "\n",
    "    elif 186 <= C <= 304.999:\n",
    "        Clow, Chigh = 186, 304\n",
    "        Ilow, Ihigh = 151, 200\n",
    "\n",
    "    elif 305 <= C <= 604.999:\n",
    "        Clow, Chigh = 305, 604\n",
    "        Ilow, Ihigh = 201, 300\n",
    "\n",
    "    else:\n",
    "        # Out of range but we extend for safety\n",
    "        return np.nan\n",
    "\n",
    "    # ===== AQI Formula =====\n",
    "    aqi = ((Ihigh - Ilow) / (Chigh - Clow)) * (C - Clow) + Ilow\n",
    "    return round(aqi, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c21cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add SO2 AQI column to DataFrame\n",
    "def add_so2_aqi_column(df, col_name=\"SO2 1st Max Value\"):\n",
    "    \"\"\"\n",
    "    df : pandas dataframe  \n",
    "    col_name : column containing SO2 1-hour max in ppb\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"SO2 AQI\"] = df[col_name].apply(calculate_so2_aqi)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_so2_aqi_column(df, \"SO2 1st Max Value\")\n",
    "df[\"SO2 AQI\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583557a",
   "metadata": {},
   "source": [
    "And check column  and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[[\"SO2 1st Max Value\", \"SO2 AQI\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec90346",
   "metadata": {},
   "source": [
    "And CO AQI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031df894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_co_aqi(C):\n",
    "    \"\"\"\n",
    "    Calculate CO AQI using expanded breakpoint intervals (Option C).\n",
    "    This ensures continuous coverage with NO missing AQI values.\n",
    "\n",
    "    C : float \n",
    "        8-hour CO concentration in ppm\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(C):\n",
    "        return np.nan\n",
    "\n",
    "    # ===== Expanded breakpoints =====\n",
    "    if 0.0 <= C <= 4.499:\n",
    "        Clow, Chigh = 0.0, 4.4\n",
    "        Ilow, Ihigh = 0, 50\n",
    "\n",
    "    elif 4.5 <= C <= 9.499:\n",
    "        Clow, Chigh = 4.5, 9.4\n",
    "        Ilow, Ihigh = 51, 100\n",
    "\n",
    "    elif 9.5 <= C <= 12.499:\n",
    "        Clow, Chigh = 9.5, 12.4\n",
    "        Ilow, Ihigh = 101, 150\n",
    "\n",
    "    elif 12.5 <= C <= 15.499:\n",
    "        Clow, Chigh = 12.5, 15.4\n",
    "        Ilow, Ihigh = 151, 200\n",
    "\n",
    "    elif 15.5 <= C <= 30.499:\n",
    "        Clow, Chigh = 15.5, 30.4\n",
    "        Ilow, Ihigh = 201, 300\n",
    "\n",
    "    else:\n",
    "        return np.nan  # extremely high or wrong units\n",
    "\n",
    "    # ===== AQI Formula =====\n",
    "    aqi = ((Ihigh - Ilow) / (Chigh - Clow)) * (C - Clow) + Ilow\n",
    "    return round(aqi, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fc5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_co_aqi_column(df, col=\"CO Mean\"):\n",
    "    df = df.copy()\n",
    "    df[\"CO AQI\"] = df[col].apply(calculate_co_aqi)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953faa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_co_aqi_column(df, \"CO Mean\")\n",
    "df[\"CO AQI\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb59af8",
   "metadata": {},
   "source": [
    "Verifying imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()           # Check for missing values in each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c392b",
   "metadata": {},
   "source": [
    "And searching for negative valeus in other numeric columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_no_negative_values(df, auto_fix=False, stop_on_error=False):\n",
    "    \"\"\"\n",
    "    Validate that no negative values exist in any numeric column.\n",
    "    \n",
    "    auto_fix: If True ‚Üí removes all rows containing negative values.\n",
    "    stop_on_error: If True ‚Üí raises an exception if negatives exist.\n",
    "    \"\"\"\n",
    "\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    negatives_report = {}\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        neg_mask = df[col] < 0\n",
    "        neg_count = neg_mask.sum()\n",
    "\n",
    "        if neg_count > 0:\n",
    "            negatives_report[col] = {\n",
    "                \"count\": int(neg_count),\n",
    "                \"sample\": df.loc[neg_mask].head()\n",
    "            }\n",
    "\n",
    "    # If clean ‚Üí report success\n",
    "    if len(negatives_report) == 0:\n",
    "        print(\"‚úÖ No negative values found in ANY numeric column.\")\n",
    "        return df\n",
    "\n",
    "    # If negatives exist ‚Üí print detailed report\n",
    "    print(\"‚ùå Negative values found!\\n\")\n",
    "\n",
    "    for col, info in negatives_report.items():\n",
    "        print(f\"Column: {col}\")\n",
    "        print(f\"  ‚Üí Negative count: {info['count']}\")\n",
    "        print(f\"  ‚Üí Sample rows with negatives:\")\n",
    "        print(info[\"sample\"])\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Auto-fix option: remove rows\n",
    "    if auto_fix:\n",
    "        print(\"üßπ Auto-fix: Removing rows containing any negative values...\")\n",
    "        df = df[(df[numeric_cols] >= 0).all(axis=1)]\n",
    "        print(\"‚úî Negative rows removed.\")\n",
    "        return df\n",
    "\n",
    "    # Stop execution option\n",
    "    if stop_on_error:\n",
    "        raise ValueError(\"Dataset contains negative values! See report above.\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_no_negative_values(df)\n",
    "df = validate_no_negative_values(df, auto_fix=True)\n",
    "df = validate_no_negative_values(df, stop_on_error=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64a3e3e",
   "metadata": {},
   "source": [
    "828 negative valeus were found in NO2 mean columns. These rows were automatically deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3134a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)                     # Print the shape of the DataFrame  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fa102",
   "metadata": {},
   "source": [
    "Checking for TRUE duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb3340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_full_pollutant_duplicates(df):\n",
    "    \"\"\"\n",
    "    Find duplicates using:\n",
    "    - Date Local\n",
    "    - Address\n",
    "    - All pollutant mean / max / hour values\n",
    "    - All AQI values\n",
    "    \n",
    "    Shows duplicates but DOES NOT remove them.\n",
    "    \"\"\"\n",
    "\n",
    "    # Identify pollutant measurement columns dynamically\n",
    "    pollutant_cols = [\n",
    "        col for col in df.columns \n",
    "        if any(p in col for p in [\"NO2\", \"O3\", \"SO2\", \"CO\"])\n",
    "    ]\n",
    "\n",
    "    # Identify AQI columns\n",
    "    aqi_cols = [col for col in df.columns if col.endswith(\"AQI\")]\n",
    "\n",
    "    # Build final set of columns for duplicate detection\n",
    "    key_columns = [\"Date Local\", \"Address\"] + pollutant_cols + aqi_cols\n",
    "\n",
    "    print(\"üîç Checking duplicates using ALL pollutant columns:\")\n",
    "    print(key_columns, \"\\n\")\n",
    "\n",
    "    # Find duplicates (count both first and later occurrences)\n",
    "    dup_mask = df.duplicated(subset=key_columns, keep=False)\n",
    "    duplicates = df.loc[dup_mask].sort_values(by=key_columns)\n",
    "\n",
    "    print(f\"üìå Total FULL pollutant duplicates found: {len(duplicates)}\\n\")\n",
    "\n",
    "    if len(duplicates) == 0:\n",
    "        print(\"‚úÖ No duplicates found.\")\n",
    "        return duplicates\n",
    "\n",
    "    print(\"üìÑ Sample of duplicate rows (first 30):\")\n",
    "    display(duplicates.head(30))\n",
    "\n",
    "    print(\"\\nüìä Duplicate groups summary:\")\n",
    "    group_counts = (\n",
    "        duplicates.groupby([\"Date Local\", \"Address\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"Count\")\n",
    "        .sort_values(\"Count\", ascending=False)\n",
    "    )\n",
    "    display(group_counts.head(20))\n",
    "\n",
    "    return duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86413770",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = find_full_pollutant_duplicates(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ef257",
   "metadata": {},
   "source": [
    "Remove only TRUE Duplicates, perfectly matching.\n",
    "\n",
    "This function:\n",
    "* Removes only exact duplicates\n",
    "*  Keeps the first occurrence\n",
    "* Shows how many were removed\n",
    "* Shows which Date/Address pairs had duplicates\n",
    "* Returns a cleaned dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fc135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_full_pollutant_duplicates(df):\n",
    "    \"\"\"\n",
    "    Remove ONLY true duplicates based on:\n",
    "    - Date Local\n",
    "    - Address\n",
    "    - All pollutant measurement columns (Mean, Max, Hour, Units)\n",
    "    - All AQI columns\n",
    "\n",
    "    Keeps the FIRST occurrence.\n",
    "    Returns cleaned dataframe + summary report.\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df.copy()\n",
    "\n",
    "    # Identify pollutant columns dynamically\n",
    "    pollutant_cols = [\n",
    "        col for col in df.columns\n",
    "        if any(p in col for p in [\"NO2\", \"O3\", \"SO2\", \"CO\"])\n",
    "    ]\n",
    "\n",
    "    # Identify AQI columns\n",
    "    aqi_cols = [col for col in df.columns if col.endswith(\"AQI\")]\n",
    "\n",
    "    key_columns = [\"Date Local\", \"Address\"] + pollutant_cols + aqi_cols\n",
    "\n",
    "    print(\"üßπ Removing true duplicates based on columns:\")\n",
    "    print(key_columns, \"\\n\")\n",
    "\n",
    "    before = len(df_clean)\n",
    "\n",
    "    # Remove duplicates (keep first occurrence)\n",
    "    df_clean = df_clean.drop_duplicates(subset=key_columns, keep=\"first\")\n",
    "\n",
    "    after = len(df_clean)\n",
    "    removed = before - after\n",
    "\n",
    "    print(f\"üìâ Total rows BEFORE: {before}\")\n",
    "    print(f\"üìà Total rows AFTER:  {after}\")\n",
    "    print(f\"üóëÔ∏è Removed duplicates: {removed}\\n\")\n",
    "\n",
    "    # Show top duplicate groups (optional)\n",
    "    if removed > 0:\n",
    "        print(\"üìä Duplicate groups (Date + Address) impacted:\")\n",
    "        dup_groups = (\n",
    "            df[df.duplicated(subset=key_columns, keep=False)]\n",
    "            .groupby([\"Date Local\", \"Address\"])\n",
    "            .size()\n",
    "            .reset_index(name=\"Count\")\n",
    "            .sort_values(\"Count\", ascending=False)\n",
    "        )\n",
    "        display(dup_groups.head(20))\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicates were removed. Dataset was already clean.\")\n",
    "\n",
    "    return df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68c7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = remove_full_pollutant_duplicates(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2788728",
   "metadata": {},
   "source": [
    "Conflicting duplicates identification:\n",
    "* Detect measurement inconsistencies\n",
    "* Identify stations that report multiple measurements at same time\n",
    "* Flag data quality problems\n",
    "* Decide whether to:\n",
    " * average conflicts\n",
    " * drop the worst sensors\n",
    " * keep the max value (common for AQI rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ce4dee",
   "metadata": {},
   "source": [
    "Resolve Conflicting Duplicates. Keep the Maximum Values (EPA-style AQI logic)\n",
    "\n",
    "EPA rules for AQI calculations already require maxima for 1-hour & 8-hour values.\n",
    "So to remain consistent, we choose the maximum values within each conflict group.\n",
    "\n",
    "* Best for AQI\n",
    "* Prevents underestimation\n",
    "* Officially aligned with U.S. EPA methodology\n",
    "\n",
    "It ensures:\n",
    "* proper pollutant selection\n",
    "* AQI is never underestimated\n",
    "* dataset integrity for environmental analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b3ad5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in df.columns if df.columns.tolist().count(x) > 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dca4c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detect_and_resolve_conflicts(df):\n",
    "    \"\"\"\n",
    "    Detect and resolve conflicting duplicates based on:\n",
    "    - Same Date Local + Address\n",
    "    - Pollutant or AQI values differ\n",
    "    Resolution rule: keep MAX values (EPA-style).\n",
    "\n",
    "    Returns:\n",
    "        df_cleaned : dataframe with resolved conflicts\n",
    "        conflicts  : dataframe containing original conflicting rows\n",
    "    \"\"\"\n",
    "\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Identify pollutant + AQI columns\n",
    "    # -----------------------------\n",
    "    pollutant_cols = [c for c in df_copy.columns if any(p in c for p in [\"NO2\", \"O3\", \"SO2\", \"CO\"])]\n",
    "    aqi_cols = [c for c in df_copy.columns if c.endswith(\"AQI\")]\n",
    "    group_cols = pollutant_cols + aqi_cols\n",
    "\n",
    "    key_cols = [\"Date Local\", \"Address\"]\n",
    "\n",
    "    print(\"üîç Checking columns:\")\n",
    "    print(\"  Pollutant cols:\", pollutant_cols)\n",
    "    print(\"  AQI cols:\", aqi_cols)\n",
    "    print(\"  Keys:\", key_cols, \"\\n\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # STEP 1 ‚Äî Detect conflicting groups\n",
    "    # -----------------------------\n",
    "    grouped = (\n",
    "        df_copy.groupby(key_cols)[group_cols]\n",
    "        .nunique(dropna=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    grouped[\"conflict\"] = grouped[group_cols].max(axis=1) > 1\n",
    "\n",
    "    conflict_keys = grouped[grouped[\"conflict\"]][key_cols]\n",
    "\n",
    "    if len(conflict_keys) == 0:\n",
    "        print(\"‚úÖ No conflicting duplicates found.\")\n",
    "        return df_copy, pd.DataFrame()\n",
    "\n",
    "    print(f\"‚ö†Ô∏è Total conflicting groups found: {len(conflict_keys)}\")\n",
    "\n",
    "    # Extract full conflicting rows\n",
    "    conflicts = df_copy.merge(conflict_keys, on=key_cols, how=\"inner\")\n",
    "    print(f\"‚ö†Ô∏è Total conflicting rows: {len(conflicts)}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # STEP 2 ‚Äî Resolve conflicts using MAX for pollutant/AQI cols\n",
    "    # -----------------------------\n",
    "    resolved_conflicts = (\n",
    "        conflicts.groupby(key_cols)[group_cols]\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # -----------------------------\n",
    "    # STEP 3 ‚Äî Merge resolved rows back into dataset\n",
    "    # -----------------------------\n",
    "    # Remove old conflicting rows\n",
    "    non_conflicts = df_copy.merge(\n",
    "        conflict_keys, on=key_cols, how=\"left\", indicator=True\n",
    "    )\n",
    "    non_conflicts = non_conflicts[non_conflicts[\"_merge\"] == \"left_only\"]\n",
    "    non_conflicts = non_conflicts.drop(columns=[\"_merge\"])\n",
    "\n",
    "    # Align column order\n",
    "    resolved_conflicts = resolved_conflicts.reindex(columns=df_copy.columns, fill_value=np.nan)\n",
    "\n",
    "    # Final dataset\n",
    "    df_cleaned = pd.concat([non_conflicts, resolved_conflicts], ignore_index=True)\n",
    "\n",
    "    print(\"\\nüõ† Conflict Resolution Summary:\")\n",
    "    print(f\"  Conflicting groups : {len(conflict_keys)}\")\n",
    "    print(f\"  Rows removed       : {len(conflicts)}\")\n",
    "    print(f\"  Rows added         : {len(resolved_conflicts)}\")\n",
    "    print(f\"  Final dataset size : {len(df_cleaned)}\")\n",
    "\n",
    "    return df_cleaned, conflicts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19d8ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_duplicate_columns(df):\n",
    "    \"\"\"\n",
    "    Ensures column names are unique by automatically renaming duplicates.\n",
    "    \"\"\"\n",
    "    new_cols = []\n",
    "    seen = {}\n",
    "\n",
    "    for col in df.columns:\n",
    "        if col not in seen:\n",
    "            new_cols.append(col)\n",
    "            seen[col] = 1\n",
    "        else:\n",
    "            new_name = f\"{col}_{seen[col]}\"\n",
    "            while new_name in seen:\n",
    "                seen[col] += 1\n",
    "                new_name = f\"{col}_{seen[col]}\"\n",
    "            new_cols.append(new_name)\n",
    "            seen[col] += 1\n",
    "\n",
    "    df.columns = new_cols\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f2073",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fix_duplicate_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b893f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned, conflicts = detect_and_resolve_conflicts(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f70888d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicts.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1a1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd3447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac3198e0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
